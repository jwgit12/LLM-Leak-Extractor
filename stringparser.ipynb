{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# File Analysis",
   "id": "37d2ff46c075980b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T20:27:14.417196Z",
     "start_time": "2025-04-10T20:27:14.414576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_lines(file_path: str, encoding: str = 'utf-8') -> int:\n",
    "    \"\"\"\n",
    "    Zählt die Anzahl der Zeilen in einer Datei.\n",
    "\n",
    "    :param file_path: Pfad zur Datei, die gezählt werden soll.\n",
    "    :param encoding: Encoding der Datei (Standard: 'utf-8').\n",
    "    :return: Anzahl der Zeilen in der Datei.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding=encoding, errors='replace') as file:\n",
    "        return sum(1 for _ in file)"
   ],
   "id": "205b098dc4b003e2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T20:27:20.645793Z",
     "start_time": "2025-04-10T20:27:15.582239Z"
    }
   },
   "cell_type": "code",
   "source": "print(count_lines('testdata'))",
   "id": "4e2a7fb010473cd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347455\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T14:44:53.242445Z",
     "start_time": "2025-04-11T14:44:53.239632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_letters(file_path: str, encoding: str = 'utf-8') -> int:\n",
    "    with open(file_path, 'r', encoding=encoding, errors='replace') as file:\n",
    "        return len(list(file.read()))"
   ],
   "id": "7953421181245144",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T14:47:31.751974Z",
     "start_time": "2025-04-11T14:45:07.035132Z"
    }
   },
   "cell_type": "code",
   "source": "print(count_letters('testdata'))",
   "id": "b02297a8a43138b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Standard String Parser",
   "id": "17a11321ff286ecb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T20:44:23.459682Z",
     "start_time": "2025-04-10T20:44:23.457162Z"
    }
   },
   "cell_type": "code",
   "source": "import re",
   "id": "5123b9445f214e7b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T21:09:54.160326Z",
     "start_time": "2025-04-10T21:09:54.156564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_credentials(file_path: str, name: str, email: str) -> dict:\n",
    "    \"\"\"\n",
    "    Durchsucht eine Datei nach einem Namen und einer E-Mail-Adresse und gibt alle Zeilennummern zurück,\n",
    "    in denen sie gefunden wurden. Die Suche ist unabhängig von Groß- und Kleinschreibung.\n",
    "\n",
    "    :param file_path: Pfad zur Datei, die durchsucht werden soll.\n",
    "    :param name: Name der Person, nach dem gesucht werden soll.\n",
    "    :param email: E-Mail-Adresse, nach der gesucht werden soll.\n",
    "    :return: Dictionary mit den Schlüsseln 'name_lines' und 'email_lines', die jeweils Listen von Zeilennummern enthalten.\n",
    "    \"\"\"\n",
    "    name_lines = []\n",
    "    email_lines = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "        for line_number, line in enumerate(file, start=1):\n",
    "            # Search for name (case insensitive)\n",
    "            if re.search(r'\\b' + re.escape(name) + r'\\b', line, re.IGNORECASE):\n",
    "                name_lines.append(line_number)\n",
    "\n",
    "            # Search for email (case insensitive)\n",
    "            if re.search(r'\\b' + re.escape(email) + r'\\b', line, re.IGNORECASE):\n",
    "                email_lines.append(line_number)\n",
    "\n",
    "    return {\n",
    "        'name_lines': name_lines,\n",
    "        'email_lines': email_lines\n",
    "    }"
   ],
   "id": "e30f16d1015e3de8",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T21:11:56.299673Z",
     "start_time": "2025-04-10T21:09:54.517878Z"
    }
   },
   "cell_type": "code",
   "source": "print(parse_credentials(\"testdata\", 'sanjay ch', 'danilgt13@gmail.com'))",
   "id": "e10fb588bbb63867",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name_lines': [26393, 394279, 427289, 1242624], 'email_lines': [1347432]}\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embedding AI approach (doen't work really well since emails have a lot of similarity in general)",
   "id": "dc34edf9fd3cece5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T21:19:11.811755Z",
     "start_time": "2025-04-10T21:19:10.010786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Union, Tuple"
   ],
   "id": "451e26fb190191ed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jannis/PycharmProjects/CyberSecurityAI/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T21:26:09.031733Z",
     "start_time": "2025-04-10T21:26:09.020117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Union, Tuple\n",
    "import time\n",
    "\n",
    "class CredentialLeakDetector:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Initialize the credential leak detector with an embedding model using GPU acceleration.\n",
    "\n",
    "        :param model_name: The name of the sentence-transformers model to use\n",
    "        :param batch_size: Batch size for processing embeddings\n",
    "        \"\"\"\n",
    "        # Check for available devices and use the best one\n",
    "        self.device = self._get_optimal_device()\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Load model to the selected device\n",
    "        self.model = SentenceTransformer(model_name, device=self.device)\n",
    "        self.batch_size = batch_size\n",
    "        self.file_lines = []\n",
    "        self.line_embeddings = None\n",
    "\n",
    "    def _get_optimal_device(self) -> str:\n",
    "        \"\"\"\n",
    "        Determine the best available device (CUDA, MPS, or CPU).\n",
    "\n",
    "        :return: Device string for torch\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "        else:\n",
    "            return \"cpu\"\n",
    "\n",
    "    def load_file(self, file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Load and preprocess the file content with performance metrics.\n",
    "\n",
    "        :param file_path: Path to the file to be analyzed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            print(f\"Loading file: {file_path}\")\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
    "                self.file_lines = file.readlines()\n",
    "\n",
    "            load_time = time.time() - start_time\n",
    "            print(f\"File loaded in {load_time:.2f} seconds with {len(self.file_lines)} lines\")\n",
    "\n",
    "            # Create embeddings for each line in the file using batching\n",
    "            print(\"Generating embeddings...\")\n",
    "            embed_start = time.time()\n",
    "\n",
    "            self.line_embeddings = self.model.encode(\n",
    "                self.file_lines,\n",
    "                batch_size=self.batch_size,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_numpy=True,\n",
    "                device=self.device\n",
    "            )\n",
    "\n",
    "            embed_time = time.time() - embed_start\n",
    "            print(f\"Embeddings generated in {embed_time:.2f} seconds\")\n",
    "\n",
    "            # Pre-normalize embeddings for faster similarity calculation later\n",
    "            norm_start = time.time()\n",
    "            self.line_embeddings = self.line_embeddings / np.linalg.norm(self.line_embeddings, axis=1, keepdims=True)\n",
    "            norm_time = time.time() - norm_start\n",
    "            print(f\"Embeddings normalized in {norm_time:.2f} seconds\")\n",
    "\n",
    "            total_time = time.time() - start_time\n",
    "            print(f\"Total preprocessing time: {total_time:.2f} seconds\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def search_credentials(self,\n",
    "                          query: str,\n",
    "                          threshold: float = 0.9,\n",
    "                          top_n: int = 5,\n",
    "                          context_size: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for credentials in the loaded file using semantic similarity.\n",
    "\n",
    "        :param query: The credential to search for (name, email, phone number)\n",
    "        :param threshold: Minimum similarity threshold (0-1)\n",
    "        :param top_n: Maximum number of results to return\n",
    "        :param context_size: Number of lines before and after the match to include\n",
    "        :return: List of dictionaries containing match information\n",
    "        \"\"\"\n",
    "        if self.line_embeddings is None:\n",
    "            raise ValueError(\"No file has been loaded. Call load_file() first.\")\n",
    "\n",
    "        search_start = time.time()\n",
    "\n",
    "        # Create embedding for the query\n",
    "        query_embedding = self.model.encode(\n",
    "            query,\n",
    "            convert_to_numpy=True,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # Normalize query embedding\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "\n",
    "        # Calculate similarity scores\n",
    "        if self.device != \"cpu\":\n",
    "            # GPU-accelerated similarity calculation\n",
    "            query_tensor = torch.tensor(query_embedding, device=self.device)\n",
    "            lines_tensor = torch.tensor(self.line_embeddings, device=self.device)\n",
    "            similarities = torch.matmul(lines_tensor, query_tensor).cpu().numpy()\n",
    "        else:\n",
    "            # CPU similarity calculation\n",
    "            similarities = np.dot(self.line_embeddings, query_embedding)\n",
    "\n",
    "        # Filter by threshold and get top matches\n",
    "        filtered_results = [(i, score) for i, score in enumerate(similarities) if score >= threshold]\n",
    "        filtered_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_results = filtered_results[:top_n]\n",
    "\n",
    "        # Extract results with context\n",
    "        detailed_results = []\n",
    "        for line_idx, similarity_score in top_results:\n",
    "            result = {\n",
    "                'line_number': line_idx + 1,  # 1-based indexing\n",
    "                'similarity_score': float(similarity_score),\n",
    "                'matched_line': self.file_lines[line_idx].strip(),\n",
    "                'context': self._extract_context(line_idx, context_size)\n",
    "            }\n",
    "            detailed_results.append(result)\n",
    "\n",
    "        search_time = time.time() - search_start\n",
    "        print(f\"Search completed in {search_time:.2f} seconds\")\n",
    "\n",
    "        return detailed_results\n",
    "\n",
    "    def _extract_context(self, line_idx: int, context_size: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract context lines around the matched line.\n",
    "\n",
    "        :param line_idx: Index of the matched line\n",
    "        :param context_size: Number of lines before and after to include\n",
    "        :return: List of context lines with line numbers\n",
    "        \"\"\"\n",
    "        start_idx = max(0, line_idx - context_size)\n",
    "        end_idx = min(len(self.file_lines), line_idx + context_size + 1)\n",
    "\n",
    "        context = []\n",
    "        for i in range(start_idx, end_idx):\n",
    "            line_info = {\n",
    "                'line_number': i + 1,  # 1-based indexing\n",
    "                'content': self.file_lines[i].strip(),\n",
    "                'is_match': i == line_idx\n",
    "            }\n",
    "            context.append(line_info)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def combined_search(self,\n",
    "                       name: str = None,\n",
    "                       email: str = None,\n",
    "                       phone: str = None,\n",
    "                       threshold: float = 0.5,\n",
    "                       top_n: int = 5) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Perform a combined search for multiple credential types.\n",
    "\n",
    "        :param name: Person's name to search for\n",
    "        :param email: Email address to search for\n",
    "        :param phone: Phone number to search for\n",
    "        :param threshold: Minimum similarity threshold\n",
    "        :param top_n: Maximum number of results to return per credential type\n",
    "        :return: Dictionary with results for each credential type\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        if name:\n",
    "            print(f\"Searching for name: {name}\")\n",
    "            results['name_matches'] = self.search_credentials(name, threshold, top_n)\n",
    "\n",
    "        if email:\n",
    "            print(f\"Searching for email: {email}\")\n",
    "            results['email_matches'] = self.search_credentials(email, threshold, top_n)\n",
    "\n",
    "        if phone:\n",
    "            print(f\"Searching for phone: {phone}\")\n",
    "            results['phone_matches'] = self.search_credentials(phone, threshold, top_n)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "def detect_credential_leaks(file_path: str,\n",
    "                           name: str = None,\n",
    "                           email: str = None,\n",
    "                           phone: str = None,\n",
    "                           threshold: float = 0.6,\n",
    "                           top_n: int = 5,\n",
    "                           batch_size: int = 64) -> Dict:\n",
    "    \"\"\"\n",
    "    Convenience function to detect credential leaks in a file with GPU acceleration.\n",
    "\n",
    "    :param file_path: Path to the file to scan\n",
    "    :param name: Person's name to search for\n",
    "    :param email: Email address to search for\n",
    "    :param phone: Phone number to search for\n",
    "    :param threshold: Minimum similarity threshold (0-1)\n",
    "    :param top_n: Maximum number of results to return per credential type\n",
    "    :param batch_size: Batch size for processing embeddings\n",
    "    :return: Dictionary with search results\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    detector = CredentialLeakDetector(batch_size=batch_size)\n",
    "    detector.load_file(file_path)\n",
    "\n",
    "    results = detector.combined_search(\n",
    "        name=name,\n",
    "        email=email,\n",
    "        phone=phone,\n",
    "        threshold=threshold,\n",
    "        top_n=top_n\n",
    "    )\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "    return results"
   ],
   "id": "742f86a60e2600ec",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T21:41:32.695545Z",
     "start_time": "2025-04-10T21:41:30.067666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "results = detect_credential_leaks(\n",
    "    file_path=\"testdata-small\",\n",
    "    name=\"sanjay ch\",\n",
    "    email=\"danilgt13@gmail.com\",\n",
    "    threshold=0.8,\n",
    "    top_n=10,\n",
    "    batch_size=128  # Adjust based on your GPU memory\n",
    ")\n",
    "\n",
    "# Print a summary of the results\n",
    "for credential_type, matches in results.items():\n",
    "    print(f\"\\n{credential_type.upper()} - {len(matches)} matches found\")\n",
    "    for i, match in enumerate(matches, 1):\n",
    "        print(f\"{i}. Line {match['line_number']} (Score: {match['similarity_score']:.2f})\")"
   ],
   "id": "7d52b0d57e9c1036",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading file: testdata-small\n",
      "File loaded in 0.00 seconds with 885 lines\n",
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:00<00:00, 14.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated in 0.47 seconds\n",
      "Embeddings normalized in 0.00 seconds\n",
      "Total preprocessing time: 0.47 seconds\n",
      "Searching for name: sanjay ch\n",
      "Search completed in 0.01 seconds\n",
      "Searching for email: danilgt13@gmail.com\n",
      "Search completed in 0.01 seconds\n",
      "Total processing time: 2.62 seconds\n",
      "\n",
      "NAME_MATCHES - 0 matches found\n",
      "\n",
      "EMAIL_MATCHES - 0 matches found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loss",
   "id": "ea4c0d619c379ca0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import json",
   "id": "99a6826290cb235a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def hallucination_check(model_output_path, dataset_path):\n",
    "    with open(model_output_path) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        leaked_credentials = json_data['leaked_credentials']\n",
    "    # create list of all emails and passwords and check if the email/password pair is in the same line and if both exist\n",
    "    \n",
    "    with open(dataset_path) as dataset_file:\n",
    "        for line_number, line in enumerate(dataset_file, start=1):\n",
    "            \n",
    "    \n",
    "        "
   ],
   "id": "4402796804f5c3a2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
